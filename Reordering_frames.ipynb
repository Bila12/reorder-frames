{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Digeiz Technical Test\n","This test consists in reordering and cleaning a video where the frames have been shuffled and some unrelated frames have been added.\n","\n","The solution here works by tracking objects across frames (using RT-DETRv2 by Baidu), removing frames where no object is detected, and then computing similarities between frames (in terms of a weighted sum of grayscale histogram distance and main bounding box tracking).\n","\n","Using this similarity metric, the algorithm then finds a path of maximization of total similarity between adjactent frames (or a path of least total distance between frames), starting from candidates frames that are the most distant on average from the other frames. After that, a coherence check removes the frames that are not coherent with their neighbours in terms of tracked objects.\n","\n","The removed frames are saved in a folder, while a cleaned video and its reversed version (as our algorithm doesn't know in which order the original video might have been filmed) are generated.\n","\n","All the infos regarding the frames and their processing are saved in a json."],"metadata":{"id":"B4eqcXCXJSzd"}},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"qYMMH2eqtDwM"}},{"cell_type":"code","source":["import cv2\n","import torch\n","import numpy as np\n","from pathlib import Path\n","from typing import Optional, Tuple, List, Dict\n","import urllib.request\n","from PIL import Image\n","from torchvision.transforms import functional as F\n","from dataclasses import dataclass\n","import json\n","import subprocess\n","import tempfile\n","import shutil\n","import os\n","from os import path"],"metadata":{"id":"zdn-yFn_sjjB","executionInfo":{"status":"ok","timestamp":1759851062331,"user_tz":-120,"elapsed":22133,"user":{"displayName":"Romain Bielawski","userId":"13459775266194102485"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### A Data Class for storing information about frames:"],"metadata":{"id":"BimH8jbXtQaY"}},{"cell_type":"code","source":["@dataclass\n","class FrameDetection:\n","    \"\"\"Store detection information for a frame.\"\"\"\n","    frame_idx: int\n","    frame: np.ndarray\n","    boxes: np.ndarray\n","    scores: np.ndarray\n","    labels: np.ndarray\n","    has_detections: bool\n","    max_confidence: float"],"metadata":{"id":"wrEP3Yqksp1-","executionInfo":{"status":"ok","timestamp":1759851062604,"user_tz":-120,"elapsed":95,"user":{"displayName":"Romain Bielawski","userId":"13459775266194102485"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### The main class for processing videos using RT-DETRv2:"],"metadata":{"id":"2IYq1WCmtdA7"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"wDu83ItyJIy3","executionInfo":{"status":"ok","timestamp":1759851062854,"user_tz":-120,"elapsed":137,"user":{"displayName":"Romain Bielawski","userId":"13459775266194102485"}}},"outputs":[],"source":["\n","\n","class RTDETRv2VideoPipeline:\n","    \"\"\"\n","    Pipeline for processing videos with RT-DETRv2 object detection.\n","    Supports filtering frames without objects and reordering shuffled frames.\n","    \"\"\"\n","\n","    def __init__(self, model_name: str = \"rtdetrv2_r50vd\", device: Optional[str] = None):\n","        \"\"\"\n","        Initialize the RT-DETRv2 video processing pipeline.\n","\n","        Args:\n","            model_name: Model variant (rtdetrv2_r18vd, rtdetrv2_r34vd, rtdetrv2_r50vd, rtdetrv2_r101vd)\n","            device: Device to run inference on ('cuda', 'cpu', or None for auto-detect)\n","        \"\"\"\n","        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.model = None\n","        self.model_name = model_name\n","        self.coco_classes = self._load_coco_classes()\n","        self.colors = self._generate_colors(len(self.coco_classes))\n","\n","    def _load_coco_classes(self):\n","        \"\"\"Load COCO dataset class names.\"\"\"\n","        self.load_model()\n","        return self.model.config.id2label\n","\n","    def _generate_colors(self, n: int):\n","        \"\"\"Generate distinct colors for each class.\"\"\"\n","        np.random.seed(42)\n","        return [(np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n","                for _ in range(n)]\n","\n","    def load_model(self, weights_path: Optional[str] = None):\n","        \"\"\"\n","        Load the RT-DETRv2 model.\n","\n","        Args:\n","            weights_path: Path to model weights. If None, uses Hugging Face transformers.\n","        \"\"\"\n","        print(f\"Loading RT-DETRv2 model on {self.device}...\")\n","\n","        if weights_path:\n","            # Load from local weights (requires RT-DETR repo)\n","            from rtdetrv2_pytorch.src.zoo.rtdetr import RTDETR\n","            self.model = RTDETR.from_pretrained(weights_path)\n","        else:\n","            # Use Hugging Face transformers (easier setup)\n","            from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n","\n","            model_map = {\n","                'rtdetrv2_r18vd': 'PekingU/rtdetr_r18vd',\n","                'rtdetrv2_r34vd': 'PekingU/rtdetr_r34vd',\n","                'rtdetrv2_r50vd': 'PekingU/rtdetr_r50vd',\n","                'rtdetrv2_r101vd': 'PekingU/rtdetr_r101vd'\n","            }\n","\n","            model_id = model_map.get(self.model_name, 'PekingU/rtdetr_r50vd')\n","            self.processor = RTDetrImageProcessor.from_pretrained(model_id)\n","            self.model = RTDetrForObjectDetection.from_pretrained(model_id)\n","\n","        self.model.to(self.device)\n","        self.model.eval()\n","        print(\"Model loaded successfully!\")\n","\n","    def preprocess_frame(self, frame: np.ndarray) -> Tuple[torch.Tensor, Tuple[int, int]]:\n","        \"\"\"\n","        Preprocess a video frame for model input.\n","\n","        Args:\n","            frame: Input frame (BGR format from OpenCV)\n","\n","        Returns:\n","            Preprocessed tensor and original size\n","        \"\"\"\n","        # Convert BGR to RGB\n","        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        pil_image = Image.fromarray(frame_rgb)\n","\n","        # Process with model processor\n","        inputs = self.processor(images=pil_image, return_tensors=\"pt\")\n","        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n","\n","        return inputs, frame.shape[:2]\n","\n","    def postprocess_predictions(self, outputs, orig_size: Tuple[int, int],\n","                               confidence_threshold: float = 0.5):\n","        \"\"\"\n","        Post-process model outputs to get bounding boxes.\n","\n","        Args:\n","            outputs: Model outputs\n","            orig_size: Original frame size (height, width)\n","            confidence_threshold: Minimum confidence for detections\n","\n","        Returns:\n","            List of detections (boxes, scores, labels)\n","        \"\"\"\n","        # Get predictions\n","        logits = outputs.logits[0]\n","        boxes = outputs.pred_boxes[0]\n","\n","        # Apply softmax to get probabilities\n","        probs = logits.softmax(-1)\n","        scores, labels = probs.max(-1)\n","\n","        # Filter by confidence\n","        keep = scores > confidence_threshold\n","        scores = scores[keep].cpu().numpy()\n","        labels = labels[keep].cpu().numpy()\n","        boxes = boxes[keep].cpu().numpy()\n","\n","        # Convert boxes from normalized [cx, cy, w, h] to [x1, y1, x2, y2]\n","        h, w = orig_size\n","        boxes_converted = boxes.copy()\n","        boxes_converted[:, 0] = (boxes[:, 0] - boxes[:, 2] / 2) * w  # x1\n","        boxes_converted[:, 1] = (boxes[:, 1] - boxes[:, 3] / 2) * h  # y1\n","        boxes_converted[:, 2] = (boxes[:, 0] + boxes[:, 2] / 2) * w  # x2\n","        boxes_converted[:, 3] = (boxes[:, 1] + boxes[:, 3] / 2) * h  # y2\n","\n","        return boxes_converted, scores, labels\n","\n","    def draw_detections(self, frame: np.ndarray, boxes, scores, labels) -> np.ndarray:\n","        \"\"\"\n","        Draw bounding boxes and labels on frame.\n","\n","        Args:\n","            frame: Input frame\n","            boxes: Bounding boxes\n","            scores: Confidence scores\n","            labels: Class labels\n","\n","        Returns:\n","            Annotated frame\n","        \"\"\"\n","        annotated_frame = frame.copy()\n","\n","        for box, score, label in zip(boxes, scores, labels):\n","            x1, y1, x2, y2 = box.astype(int)\n","            color = self.colors[label]\n","\n","            # Draw bounding box\n","            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n","\n","            # Prepare label text\n","            class_name = self.coco_classes[label]\n","            text = f\"{class_name}: {score:.2f}\"\n","\n","            # Draw label background\n","            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n","            cv2.rectangle(annotated_frame, (x1, y1 - text_height - 10),\n","                         (x1 + text_width, y1), color, -1)\n","\n","            # Draw label text\n","            cv2.putText(annotated_frame, text, (x1, y1 - 5),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n","\n","        return annotated_frame\n","\n","    def compute_frame_similarity(self, frame1: np.ndarray, frame2: np.ndarray) -> float:\n","        \"\"\"\n","        Compute similarity between two frames using histogram comparison.\n","\n","        Args:\n","            frame1: First frame\n","            frame2: Second frame\n","\n","        Returns:\n","            Similarity score (0-1, higher is more similar)\n","        \"\"\"\n","        # Convert to grayscale\n","        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n","        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n","\n","        # Compute histograms\n","        hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])\n","        hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])\n","\n","        # Normalize histograms\n","        hist1 = cv2.normalize(hist1, hist1).flatten()\n","        hist2 = cv2.normalize(hist2, hist2).flatten()\n","\n","        # Compute correlation\n","        correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n","\n","        return correlation\n","\n","    def check_bbox_coherence(self, fd1: FrameDetection, fd2: FrameDetection,\n","                            target_class: Optional[int] = None,\n","                            iou_threshold: float = 0.3,\n","                            check_all_objects: bool = True) -> bool:\n","        \"\"\"\n","        Check if bounding boxes are coherent between two adjacent frames.\n","\n","        Args:\n","            fd1: First frame detection\n","            fd2: Second frame detection\n","            target_class: Class ID to prioritize (or None to check all)\n","            iou_threshold: Minimum IoU to consider boxes coherent\n","            check_all_objects: If True, check all common objects; if False, only target_class\n","\n","        Returns:\n","            True if boxes are coherent, False otherwise\n","        \"\"\"\n","        if check_all_objects or target_class is None:\n","            # Check if ANY objects are consistent between frames (more robust)\n","            # Group boxes by class\n","            boxes_by_class_1 = {}\n","            for box, label in zip(fd1.boxes, fd1.labels):\n","                label_int = int(label)\n","                if label_int not in boxes_by_class_1:\n","                    boxes_by_class_1[label_int] = []\n","                boxes_by_class_1[label_int].append(box)\n","\n","            boxes_by_class_2 = {}\n","            for box, label in zip(fd2.boxes, fd2.labels):\n","                label_int = int(label)\n","                if label_int not in boxes_by_class_2:\n","                    boxes_by_class_2[label_int] = []\n","                boxes_by_class_2[label_int].append(box)\n","\n","            # Find common object classes between frames\n","            common_classes = set(boxes_by_class_1.keys()) & set(boxes_by_class_2.keys())\n","\n","            if not common_classes:\n","                # No common objects - frames are incoherent\n","                return False\n","\n","\n","            # Check each common class - if ANY has good IoU, frames are coherent\n","            classes_to_check = common_classes\n","            max_iou_overall = 0\n","            best_class = None\n","\n","            for cls in classes_to_check:\n","                boxes1 = boxes_by_class_1[cls]\n","                boxes2 = boxes_by_class_2[cls]\n","\n","                # Find best matching box for this class\n","                max_iou_class = 0\n","                for b1 in boxes1:\n","                    for b2 in boxes2:\n","                        iou = self.compute_bbox_iou(b1, b2)\n","                        if iou > max_iou_class:\n","                            max_iou_class = iou\n","\n","                if max_iou_class > max_iou_overall:\n","                    max_iou_overall = max_iou_class\n","                    best_class = cls\n","\n","                # Early exit if we found a good match\n","                if max_iou_overall >= iou_threshold:\n","                    return True\n","\n","            # No class had sufficient IoU\n","            return False\n","\n","        else:\n","\n","            # Get boxes for target class in both frames\n","            boxes1 = [box for box, label in zip(fd1.boxes, fd1.labels) if int(label) == target_class]\n","            boxes2 = [box for box, label in zip(fd2.boxes, fd2.labels) if int(label) == target_class]\n","\n","            # If target object missing in either frame\n","            if not boxes1 or not boxes2:\n","                return False\n","\n","            # Find best matching box (highest IoU)\n","            max_iou = 0\n","            for b1 in boxes1:\n","                for b2 in boxes2:\n","                    iou = self.compute_bbox_iou(b1, b2)\n","                    max_iou = max(max_iou, iou)\n","\n","            return max_iou >= iou_threshold\n","\n","    def filter_incoherent_frames(self, frame_detections: List[FrameDetection],\n","                                 target_class: Optional[int] = None,\n","                                 iou_threshold: float = 0.3,\n","                                 window_size: int = 2,\n","                                 check_all_objects: bool = True) -> List[FrameDetection]:\n","        \"\"\"\n","        Filter out frames with incoherent bounding boxes compared to adjacent frames.\n","\n","        Args:\n","            frame_detections: Ordered list of FrameDetection objects\n","            target_class: Target object class to prioritize (or None)\n","            iou_threshold: Minimum IoU between adjacent frames to keep frame\n","            window_size: Number of adjacent frames to check (1=prev only, 2=prev+next)\n","            check_all_objects: If True, check ALL common objects between frames (recommended);\n","                              If False, only check target_class (strict tracking)\n","\n","        Returns:\n","            Filtered list of FrameDetection objects\n","        \"\"\"\n","        if len(frame_detections) <= 2:\n","            return frame_detections\n","\n","        print(f\"Filtering frames with incoherent bounding boxes...\")\n","        print(f\"  IoU threshold: {iou_threshold}, Window size: {window_size}\")\n","        print(f\"  Check all objects: {check_all_objects}\")\n","\n","        # Analyze object distribution\n","        if check_all_objects or target_class is None:\n","            all_classes = set()\n","            for fd in frame_detections:\n","                all_classes.update([int(l) for l in fd.labels])\n","            print(f\"  Total unique object types in sequence: {len(all_classes)}\")\n","            print(f\"  Object types: {[self.coco_classes[c] for c in sorted(all_classes)]}\")\n","\n","        # Mark frames as coherent or incoherent\n","        keep_flags = [False] * len(frame_detections)\n","\n","        # Check each frame against its neighbors\n","        for i in range(len(frame_detections)):\n","            coherence_count = 0\n","            checks = 0\n","\n","            # Check against previous frame\n","            if i > 0:\n","                if self.check_bbox_coherence(frame_detections[i-1], frame_detections[i],\n","                                            target_class, iou_threshold, check_all_objects):\n","                    coherence_count += 1\n","                checks += 1\n","\n","            # Check against next frame (if window_size >= 2)\n","            if window_size >= 2 and i < len(frame_detections) - 1:\n","                if self.check_bbox_coherence(frame_detections[i], frame_detections[i+1],\n","                                            target_class, iou_threshold, check_all_objects):\n","                    coherence_count += 1\n","                checks += 1\n","\n","            # Keep frame if it's coherent with at least one neighbor\n","            if checks > 0 and coherence_count > 0:\n","                keep_flags[i] = True\n","\n","        # Ensure we keep at least some frames\n","        if sum(keep_flags) == 0:\n","            print(\"  WARNING: All frames marked as incoherent! Keeping all frames.\")\n","            return frame_detections\n","\n","        # Filter frames\n","        filtered = [fd for fd, keep in zip(frame_detections, keep_flags) if keep]\n","        removed = len(frame_detections) - len(filtered)\n","\n","        print(f\"  Removed {removed} incoherent frames\")\n","        print(f\"  Kept {len(filtered)} coherent frames\")\n","\n","        # Show which frames were removed\n","        if removed > 0 and removed <= 20:\n","            removed_indices = [fd.frame_idx for fd, keep in zip(frame_detections, keep_flags) if not keep]\n","            print(f\"  Removed frame indices: {removed_indices}\")\n","\n","        return filtered\n","\n","    def compute_bbox_iou(self, box1: np.ndarray, box2: np.ndarray) -> float:\n","        \"\"\"\n","        Compute Intersection over Union (IoU) between two bounding boxes.\n","\n","        Args:\n","            box1: First box [x1, y1, x2, y2]\n","            box2: Second box [x1, y1, x2, y2]\n","\n","        Returns:\n","            IoU score (0-1, higher means more overlap)\n","        \"\"\"\n","        # Calculate intersection area\n","        x1 = max(box1[0], box2[0])\n","        y1 = max(box1[1], box2[1])\n","        x2 = min(box1[2], box2[2])\n","        y2 = min(box1[3], box2[3])\n","\n","        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n","\n","        # Calculate union area\n","        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","        union = box1_area + box2_area - intersection\n","\n","        if union == 0:\n","            return 0.0\n","\n","        return intersection / union\n","\n","    def find_most_present_object(self, frame_detections: List[FrameDetection]) -> Tuple[int, int]:\n","        \"\"\"\n","        Find the most frequently detected object class and its typical bounding box.\n","\n","        Args:\n","            frame_detections: List of FrameDetection objects\n","\n","        Returns:\n","            Tuple of (most_common_class_id, frame_index_with_best_example)\n","        \"\"\"\n","        from collections import Counter\n","\n","        # Count all detected classes\n","        class_counter = Counter()\n","        class_boxes = {}  # Store boxes for each class\n","\n","        for fd in frame_detections:\n","            for label, box, score in zip(fd.labels, fd.boxes, fd.scores):\n","                label = int(label)\n","                class_counter[label] += 1\n","\n","                if label not in class_boxes:\n","                    class_boxes[label] = []\n","                class_boxes[label].append({\n","                    'frame_idx': fd.frame_idx,\n","                    'box': box,\n","                    'score': score\n","                })\n","\n","        if not class_counter:\n","            return None, None\n","\n","        # Get most common class\n","        most_common_class = class_counter.most_common(1)[0][0]\n","\n","        # Find the frame with the highest confidence detection of this class\n","        best_detection = max(class_boxes[most_common_class], key=lambda x: x['score'])\n","        best_frame_idx = best_detection['frame_idx']\n","\n","        # Find which frame_detection corresponds to this frame_idx\n","        for idx, fd in enumerate(frame_detections):\n","            if fd.frame_idx == best_frame_idx:\n","                return most_common_class, idx\n","\n","        return most_common_class, 0\n","\n","    def compute_object_tracking_score(self, fd1: FrameDetection, fd2: FrameDetection,\n","                                     target_class: int) -> float:\n","        #TODO: Multiple object tracking score (instead of main object across frames)\n","        \"\"\"\n","        Compute a score for how well fd2 follows fd1 based on tracking the target object.\n","\n","        Args:\n","            fd1: First frame detection\n","            fd2: Second frame detection\n","            target_class: Class ID to track\n","\n","        Returns:\n","            Tracking score (higher is better)\n","        \"\"\"\n","        # Get boxes for target class in both frames\n","        boxes1 = [box for box, label in zip(fd1.boxes, fd1.labels) if int(label) == target_class]\n","        boxes2 = [box for box, label in zip(fd2.boxes, fd2.labels) if int(label) == target_class]\n","\n","        if not boxes1 or not boxes2:\n","            # If target object not in one or both frames, use visual similarity\n","            return self.compute_frame_similarity(fd1.frame, fd2.frame) * 0.5\n","\n","        # Find best matching box (highest IoU)\n","        max_iou = 0\n","        for b1 in boxes1:\n","            for b2 in boxes2:\n","                iou = self.compute_bbox_iou(b1, b2)\n","                max_iou = max(max_iou, iou)\n","\n","        # Combine IoU with visual similarity\n","        visual_sim = self.compute_frame_similarity(fd1.frame, fd2.frame)\n","\n","        # Weighted combination: prioritize object tracking (IoU) over visual similarity\n","        score = 0.7 * max_iou + 0.3 * visual_sim\n","\n","        return score\n","\n","    def find_candidate_start_frames(self, frame_detections: List[FrameDetection],\n","                                   similarity_matrix: np.ndarray,\n","                                   num_candidates: int = 5) -> List[int]:\n","        \"\"\"\n","        Find candidate frames to use as starting points for TSP optimization.\n","\n","        Args:\n","            frame_detections: List of FrameDetection objects\n","            similarity_matrix: Precomputed pairwise similarity matrix\n","            num_candidates: Number of candidate starting frames to return\n","\n","        Returns:\n","            List of frame indices to try as starting points\n","        \"\"\"\n","        n = len(frame_detections)\n","        candidates = []\n","\n","        # Select frames that are MOST DIFFERENT from others (high avg distance)\n","        # These are good starting points as they're at the \"edges\" of the sequence\n","        avg_distances = []\n","        for i in range(n):\n","            # Calculate average dissimilarity (1 - similarity) to all other frames\n","            avg_dist = np.mean(1 - similarity_matrix[i])\n","            avg_distances.append((i, avg_dist))\n","\n","        # Sort by average distance (descending) - most different frames first\n","        avg_distances.sort(key=lambda x: x[1], reverse=True)\n","\n","        # Take top diverse candidates\n","        candidates = [idx for idx, _ in avg_distances[:num_candidates]]\n","        print(f\"    Selected diverse frames (high avg distance from others)\")\n","\n","        # Ensure we have enough candidates\n","        if len(candidates) < num_candidates:\n","            # Add some evenly distributed frames\n","            step = n // (num_candidates - len(candidates) + 1)\n","            for i in range(0, n, step):\n","                if i not in candidates and len(candidates) < num_candidates:\n","                    candidates.append(i)\n","\n","        return candidates[:num_candidates]\n","\n","    def reorder_frames(self, frame_detections: List[FrameDetection],\n","                      use_object_tracking: bool = True,\n","                      num_start_candidates: int = 5) -> List[FrameDetection]:\n","        \"\"\"\n","        Reorder shuffled frames by optimizing the total similarity between adjacent frames.\n","        Tries multiple starting frames and selects the best solution.\n","\n","        Args:\n","            frame_detections: List of FrameDetection objects\n","            use_object_tracking: If True, track the most present object for reordering\n","            num_start_candidates: Number of different starting frames to try\n","\n","        Returns:\n","            Reordered list of FrameDetection objects\n","        \"\"\"\n","        if len(frame_detections) <= 1:\n","            return frame_detections\n","\n","        print(\"Reordering shuffled frames using TSP optimization with multiple starting points...\")\n","\n","        # Find the most present object to track\n","        target_class = None\n","\n","        if use_object_tracking:\n","            target_class, _ = self.find_most_present_object(frame_detections)\n","            if target_class is not None:\n","                class_name = self.coco_classes[target_class]\n","                print(f\"  Tracking most present object: '{class_name}' (class {target_class})\")\n","            else:\n","                print(\"  No objects detected for tracking, using visual similarity only\")\n","                use_object_tracking = False\n","\n","        n = len(frame_detections)\n","        print(f\"  Computing similarity matrix for {n} frames...\")\n","\n","        # Compute pairwise similarity matrix (cache all similarities)\n","        similarity_matrix = np.zeros((n, n))\n","\n","        for i in range(n):\n","            if i % 20 == 0:\n","                print(f\"    Computing similarities: {i}/{n} frames...\")\n","            for j in range(i + 1, n):\n","                if use_object_tracking and target_class is not None:\n","                    score = self.compute_object_tracking_score(\n","                        frame_detections[i], frame_detections[j], target_class\n","                    )\n","                else:\n","                    score = self.compute_frame_similarity(\n","                        frame_detections[i].frame, frame_detections[j].frame\n","                    )\n","                similarity_matrix[i][j] = score\n","                similarity_matrix[j][i] = score  # Symmetric\n","\n","        print(f\"  Similarity matrix computed!\")\n","\n","        # Get candidate starting frames using the specified strategy\n","        print(f\"  Finding candidate starting frames...\")\n","        candidate_starts = self.find_candidate_start_frames(\n","            frame_detections,\n","            similarity_matrix,  # Pass the similarity matrix\n","            num_start_candidates,\n","        )\n","\n","        print(f\"  Trying {len(candidate_starts)} different starting frames...\")\n","\n","        # Try TSP with each starting frame\n","        best_route = None\n","        best_score = -float('inf')\n","        best_start = None\n","\n","        for candidate_idx, start_idx in enumerate(candidate_starts):\n","            print(f\"\\n  Candidate {candidate_idx + 1}/{len(candidate_starts)}: Starting from frame {frame_detections[start_idx].frame_idx}\")\n","\n","            print(f\"    Using nearest neighbours + 2-opt optimization...\")\n","            route = self.tsp_nearest_neighbor_2opt(similarity_matrix, start_idx, max_iterations=1000)\n","\n","            # Calculate total score for this route\n","            total_score = sum(similarity_matrix[route[i]][route[i+1]]\n","                            for i in range(len(route) - 1))\n","            avg_score = total_score / (len(route) - 1)\n","\n","            print(f\"    Total score: {total_score:.2f}, Average: {avg_score:.3f}\")\n","\n","            if total_score > best_score:\n","                best_score = total_score\n","                best_route = route\n","                best_start = start_idx\n","                print(f\"    ✓ New best solution!\")\n","\n","        # Reorder frames according to optimal path\n","        print(f\"\\n  Best starting frame: {frame_detections[best_start].frame_idx}\")\n","        print(f\"  Best total similarity score: {best_score:.2f}\")\n","        print(f\"  Best average adjacent frame similarity: {best_score / (n - 1):.3f}\")\n","\n","        ordered = [frame_detections[i] for i in best_route]\n","\n","        print(f\"Frame reordering complete!\")\n","\n","        return ordered\n","\n","    def tsp_nearest_neighbor_2opt(self, similarity_matrix: np.ndarray, start: int,\n","                  max_iterations: int = 1000) -> List[int]:\n","        \"\"\"\n","        2-opt local search for TSP. Good balance of quality and speed.\n","        \"\"\"\n","        n = len(similarity_matrix)\n","\n","        # Start with nearest neighbor solution\n","        route = self._nearest_neighbor_tour(similarity_matrix, start)\n","\n","        def calculate_total_distance(route):\n","            return -sum(similarity_matrix[route[i]][route[i+1]]\n","                       for i in range(len(route) - 1))\n","\n","        best_distance = calculate_total_distance(route)\n","        improved = True\n","        iteration = 0\n","\n","        while improved and iteration < max_iterations:\n","            improved = False\n","            iteration += 1\n","\n","            for i in range(1, n - 2):\n","                for j in range(i + 1, n - 1):\n","                    # Try reversing segment [i:j+1]\n","                    new_route = route[:i] + route[i:j+1][::-1] + route[j+1:]\n","                    new_distance = calculate_total_distance(new_route)\n","\n","                    if new_distance < best_distance:\n","                        route = new_route\n","                        best_distance = new_distance\n","                        improved = True\n","                        break\n","\n","                if improved:\n","                    break\n","\n","            if iteration % 100 == 0:\n","                print(f\"    2-opt iteration {iteration}, score: {-best_distance:.2f}\")\n","\n","        print(f\"    2-opt converged after {iteration} iterations\")\n","        return route\n","\n","\n","    def _nearest_neighbor_tour(self, similarity_matrix: np.ndarray, start: int) -> List[int]:\n","        \"\"\"\n","        Greedy nearest neighbor heuristic for TSP.\n","        \"\"\"\n","        n = len(similarity_matrix)\n","        unvisited = set(range(n))\n","        route = [start]\n","        unvisited.remove(start)\n","\n","        current = start\n","        while unvisited:\n","            # Find nearest unvisited node (highest similarity)\n","            next_node = max(unvisited, key=lambda x: similarity_matrix[current][x])\n","            route.append(next_node)\n","            unvisited.remove(next_node)\n","            current = next_node\n","\n","        return route\n","\n","    def process_video_with_filtering(self,\n","                                    input_path: str,\n","                                    output_path: str,\n","                                    confidence_threshold: float = 0.5,\n","                                    remove_empty_frames: bool = True,\n","                                    reorder_shuffled: bool = False,\n","                                    filter_incoherent: bool = False,\n","                                    bbox_iou_threshold: float = 0.3,\n","                                    draw_bboxes: bool = True,\n","                                    generate_reversed: bool = False,\n","                                    save_removed_frames: bool = False,\n","                                    removed_frames_folder: str = \"removed_frames\",\n","                                    save_metadata: bool = True,\n","                                    show_progress: bool = True):\n","        \"\"\"\n","        Process a video with object detection, filtering, and optional reordering.\n","\n","        Args:\n","            input_path: Path to input video\n","            output_path: Path to save output video\n","            confidence_threshold: Minimum confidence for detections\n","            remove_empty_frames: Remove frames without detected objects\n","            reorder_shuffled: Reorder frames if they are shuffled\n","            filter_incoherent: Remove frames with incoherent bounding boxes after reordering\n","            bbox_iou_threshold: Minimum IoU between adjacent frames for coherence check\n","            draw_bboxes: Draw bounding boxes on output frames\n","            generate_reversed: Also generate a reversed version of the video\n","            save_removed_frames: Save removed frames as images to a folder\n","            removed_frames_folder: Folder path for saving removed frames\n","            save_metadata: Save detection metadata to JSON\n","            show_progress: Display progress information\n","        \"\"\"\n","        if self.model is None:\n","            self.load_model()\n","\n","\n","        # Determine output path\n","        temp_output = None\n","        final_output = output_path\n","\n","\n","        output_path_for_cv = output_path\n","        # Ensure output has .mp4 extension\n","        if not output_path_for_cv.lower().endswith('.mp4'):\n","            output_path_for_cv = output_path_for_cv.rsplit('.', 1)[0] + '.mp4'\n","\n","        # Open video\n","        cap = cv2.VideoCapture(input_path)\n","\n","        if not cap.isOpened():\n","            raise ValueError(f\"Could not open video: {input_path}\")\n","\n","        # Get video properties\n","        fps = int(cap.get(cv2.CAP_PROP_FPS))\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        print(f\"Processing video: {input_path}\")\n","        print(f\"Total frames: {total_frames}, FPS: {fps}, Resolution: {width}x{height}\")\n","        print(f\"Settings: remove_empty={remove_empty_frames}, reorder={reorder_shuffled}\")\n","\n","        # Process all frames and store detections\n","        frame_detections = []\n","        frame_count = 0\n","\n","        print(\"\\nPhase 1: Detecting objects in all frames...\")\n","\n","        try:\n","            while True:\n","                ret, frame = cap.read()\n","\n","                if not ret:\n","                    break\n","\n","                # Run detection on every frame\n","                with torch.no_grad():\n","                    inputs, orig_size = self.preprocess_frame(frame)\n","                    outputs = self.model(**inputs)\n","                    boxes, scores, labels = self.postprocess_predictions(\n","                        outputs, orig_size, confidence_threshold\n","                    )\n","\n","                # Store detection info\n","                has_detections = len(boxes) > 0\n","                max_conf = float(np.max(scores)) if has_detections else 0.0\n","\n","                frame_detection = FrameDetection(\n","                    frame_idx=frame_count,\n","                    frame=frame.copy(),\n","                    boxes=boxes,\n","                    scores=scores,\n","                    labels=labels,\n","                    has_detections=has_detections,\n","                    max_confidence=max_conf\n","                )\n","\n","                frame_detections.append(frame_detection)\n","                frame_count += 1\n","\n","                if show_progress and frame_count % 30 == 0:\n","                    progress = (frame_count / total_frames) * 100\n","                    detected = sum(1 for fd in frame_detections if fd.has_detections)\n","                    print(f\"  Progress: {progress:.1f}% ({frame_count}/{total_frames} frames, {detected} with detections)\")\n","\n","        finally:\n","            cap.release()\n","\n","        # Filter frames without detections if requested\n","        if remove_empty_frames:\n","            print(f\"\\nPhase 2: Filtering frames without detections...\")\n","            original_count = len(frame_detections)\n","\n","            # Separate kept and removed frames\n","            kept_frames = []\n","            removed_frames = []\n","            removed_frames_info = []\n","\n","            for fd in frame_detections:\n","                if fd.has_detections:\n","                    kept_frames.append(fd)\n","                else:\n","                    removed_frames.append(fd)\n","\n","            removed_count = len(removed_frames)\n","            print(f\"  Removed {removed_count} frames without detections\")\n","            print(f\"  Kept {len(kept_frames)} frames with detections\")\n","\n","            # Save removed frames if requested\n","            if save_removed_frames and removed_frames:\n","                print(f\"  Saving {len(removed_frames)} removed frames to {removed_frames_folder}/...\")\n","                if path.exists(removed_frames_folder) == False:\n","                    os.mkdir(removed_frames_folder)\n","                for fd in removed_frames:\n","                    filename = f\"removed_empty_frame_{fd.frame_idx:06d}.jpg\"\n","                    filepath = os.path.join(removed_frames_folder, filename)\n","                    cv2.imwrite(filepath, fd.frame)\n","                    removed_frames_info.append({\n","                        'original_frame_idx': fd.frame_idx,\n","                        'reason': 'no_detections',\n","                        'saved_path': filepath\n","                    })\n","                print(f\"  ✓ Saved empty frames\")\n","\n","            frame_detections = kept_frames\n","\n","        # Reorder frames if requested\n","        if reorder_shuffled and len(frame_detections) > 1:\n","            print(f\"\\nPhase 3: Reordering shuffled frames...\")\n","            frame_detections = self.reorder_frames(\n","                frame_detections,\n","                use_object_tracking=True,\n","                num_start_candidates=5,  # Try 5 different starting frames\n","            )\n","\n","        # Filter incoherent frames if requested\n","        if filter_incoherent and len(frame_detections) > 2:\n","            phase_num = 4 if reorder_shuffled else 3\n","            print(f\"\\nPhase {phase_num}: Filtering frames with incoherent bounding boxes...\")\n","\n","            # Keep track of which frames to remove\n","            original_count = len(frame_detections)\n","\n","            filtered_frames = self.filter_incoherent_frames(\n","                frame_detections,\n","                target_class=None,  # Will auto-detect most common class\n","                iou_threshold=bbox_iou_threshold,\n","                window_size=2,  # Check both previous and next frames\n","                check_all_objects=True  # Check ANY common objects (not just primary)\n","            )\n","\n","            # Find removed frames\n","            if save_removed_frames:\n","                kept_indices = {fd.frame_idx for fd in filtered_frames}\n","                removed_incoherent = [fd for fd in frame_detections if fd.frame_idx not in kept_indices]\n","\n","                if removed_incoherent:\n","                    print(f\"  Saving {len(removed_incoherent)} incoherent frames to {removed_frames_folder}/...\")\n","                    for fd in removed_incoherent:\n","                        filename = f\"removed_incoherent_frame_{fd.frame_idx:06d}.jpg\"\n","                        filepath = os.path.join(removed_frames_folder, filename)\n","\n","                        # Draw bboxes on saved frame to show why it was removed\n","                        annotated = self.draw_detections(fd.frame, fd.boxes, fd.scores, fd.labels)\n","                        cv2.imwrite(filepath, annotated)\n","\n","                        removed_frames_info.append({\n","                            'original_frame_idx': fd.frame_idx,\n","                            'reason': 'incoherent_bboxes',\n","                            'num_detections': len(fd.boxes),\n","                            'detected_classes': [self.coco_classes[int(label)] for label in fd.labels],\n","                            'saved_path': filepath\n","                        })\n","                    print(f\"  ✓ Saved incoherent frames with bounding boxes\")\n","\n","            frame_detections = filtered_frames\n","\n","        # Write output video\n","        phase_num = 3\n","        if reorder_shuffled:\n","            phase_num += 1\n","        if filter_incoherent:\n","            phase_num += 1\n","        print(f\"\\nPhase {phase_num}: Writing output video...\")\n","\n","        # Helper function to write video\n","        def write_video(frame_list, output_file, video_label=\"output\"):\n","            \"\"\"Write frames to video file.\"\"\"\n","            temp_out = None\n","            final_out = output_file\n","\n","            out_file = output_file\n","            if not out_file.lower().endswith('.mp4'):\n","                out_file = out_file.rsplit('.', 1)[0] + '.mp4'\n","\n","            # Create video writer\n","            codecs_to_try = [('avc1', 'H.264'), ('mp4v', 'MPEG-4'), ('X264', 'x264')]\n","            out_writer = None\n","            for codec, name in codecs_to_try:\n","                fourcc = cv2.VideoWriter_fourcc(*codec)\n","                out_writer = cv2.VideoWriter(out_file, fourcc, fps, (width, height))\n","                if out_writer.isOpened():\n","                    print(f\"    Using codec: {name} ({codec})\")\n","                    break\n","                out_writer.release()\n","\n","            if out_writer is None or not out_writer.isOpened():\n","                raise RuntimeError(\"Could not create video writer\")\n","\n","            if not out_writer.isOpened():\n","                raise RuntimeError(\"Could not create video writer\")\n","\n","            # Write frames\n","            for idx, fd in enumerate(frame_list):\n","                # Draw detections if requested\n","                if draw_bboxes:\n","                    output_frame = self.draw_detections(fd.frame, fd.boxes, fd.scores, fd.labels)\n","                else:\n","                    output_frame = fd.frame\n","\n","                out_writer.write(output_frame)\n","\n","                if show_progress and (idx + 1) % 50 == 0:\n","                    progress = ((idx + 1) / len(frame_list)) * 100\n","                    print(f\"    Writing {video_label}: {progress:.1f}% ({idx + 1}/{len(frame_list)} frames)\")\n","\n","            out_writer.release()\n","\n","            return final_out\n","\n","        # Write forward video\n","        print(f\"  Writing forward video...\")\n","        final_output = write_video(frame_detections, output_path, \"forward video\")\n","\n","        # Write reversed video if requested\n","        reversed_output = None\n","        if generate_reversed:\n","            print(f\"\\n  Writing reversed video...\")\n","            reversed_path = output_path.rsplit('.', 1)[0] + '_reversed.mp4'\n","            reversed_frames = list(reversed(frame_detections))\n","            reversed_output = write_video(reversed_frames, reversed_path, \"reversed video\")\n","\n","        metadata = {\n","            'input_video': input_path,\n","            'output_video': final_output,\n","            'reversed_video': reversed_output,\n","            'original_frames': total_frames,\n","            'output_frames': len(frame_detections),\n","            'removed_frames': total_frames - len(frame_detections),\n","            'removed_frames_saved': save_removed_frames,\n","            'removed_frames_folder': removed_frames_folder if save_removed_frames else None,\n","            'fps': fps,\n","            'resolution': [width, height],\n","            'confidence_threshold': confidence_threshold,\n","            'draw_bboxes': draw_bboxes,\n","            'frames': [],\n","            'removed_frames_details': removed_frames_info if save_removed_frames else []\n","        }\n","\n","        for idx, fd in enumerate(frame_detections):\n","            if save_metadata:\n","                frame_meta = {\n","                    'output_frame_idx': idx,\n","                    'original_frame_idx': fd.frame_idx,\n","                    'num_detections': len(fd.boxes),\n","                    'max_confidence': float(fd.max_confidence),\n","                    'detected_classes': [self.coco_classes[int(label)] for label in fd.labels]\n","                }\n","                metadata['frames'].append(frame_meta)\n","\n","        # Save metadata\n","        if save_metadata:\n","            metadata_path = final_output.rsplit('.', 1)[0] + '_metadata.json'\n","            with open(metadata_path, 'w') as f:\n","                json.dump(metadata, f, indent=2)\n","            print(f\"\\nMetadata saved to: {metadata_path}\")\n","\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Video processing complete!\")\n","        print(f\"Original frames: {total_frames}\")\n","        print(f\"Output frames: {len(frame_detections)}\")\n","        print(f\"Frames removed: {total_frames - len(frame_detections)}\")\n","        if save_removed_frames and removed_frames_info:\n","            print(f\"Removed frames saved to: {removed_frames_folder}/ ({len(removed_frames_info)} images)\")\n","        print(f\"Bounding boxes drawn: {'Yes' if draw_bboxes else 'No'}\")\n","        print(f\"Forward video saved to: {final_output}\")\n","        if generate_reversed:\n","            print(f\"Reversed video saved to: {reversed_output}\")\n","        print(f\"{'='*60}\")\n","\n","\n"]},{"cell_type":"markdown","source":["### Now let's instantiate the pipeline with the model..."],"metadata":{"id":"XgMEpBXCtkrF"}},{"cell_type":"code","source":["# Initialize pipeline (model should be in [rtdetrv2_r18vd, rtdetrv2_r34vd, rtdetrv2_r50vd, rtdetrv2_r101vd])\n","pipeline = RTDETRv2VideoPipeline(\n","    model_name=\"rtdetrv2_r50vd\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHuWKF-huGXd","outputId":"a12f1a37-ca91-4dd3-b723-6e2dd3ebc142"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading RT-DETRv2 model on cpu...\n"]}]},{"cell_type":"markdown","source":["### And run it (don't forget to upload the corrupted video to the environment!)"],"metadata":{"id":"Ce1q1jDNuFY2"}},{"cell_type":"code","source":["# Process video with filtering and reordering\n","pipeline.process_video_with_filtering(\n","    input_path=\"/content/corrupted_video.mp4\",\n","    output_path=\"corrected_video.mp4\",\n","    confidence_threshold=0.5,\n","    remove_empty_frames=True,   # Remove frames without objects\n","    reorder_shuffled=True,       # Reorder shuffled frames (tries 5 starting points)\n","    filter_incoherent=True,      # Remove frames with inconsistent bounding boxes\n","    bbox_iou_threshold=0.3,      # Min IoU between adjacent frames (0.2-0.5 typical)\n","    draw_bboxes=True,            # Draw bounding boxes on output (set False for clean video)\n","    generate_reversed=True,      # Also generate reversed video\n","    save_removed_frames=True,    # Save removed frames as images\n","    removed_frames_folder=\"/content/removed_frames\",  # Folder for removed frames\n","    save_metadata=True,          # Save frame metadata to JSON\n","    show_progress=True\n",")\n","\n","    # Note: The pipeline has these main phases:\n","    # 1. Detect objects in all frames\n","    # 2. Remove frames without detections (optional) - saved to folder\n","    # 3. Reorder shuffled frames using TSP (optional)\n","    # 4. Filter frames with incoherent bounding boxes (optional) - saved to folder\n","    # 5. Write output video(s) - forward and/or reversed\n","    #\n","    # Output files:\n","    # - output_video.mp4: Forward video (with or without bboxes)\n","    # - output_video_reversed.mp4: Reversed video (if generate_reversed=True)\n","    # - output_video_metadata.json: Frame metadata (if save_metadata=True)\n","    # - removed_frames/: Folder with removed frame images (if save_removed_frames=True)\n","    #   - removed_empty_frame_XXXXXX.jpg: Frames without detections\n","    #   - removed_incoherent_frame_XXXXXX.jpg: Frames with incoherent bboxes (annotated)"],"metadata":{"id":"4dTTkAKNXVaV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The resulting videos can now be downloaded from the environment. And you can check which frames were removed in the dedicated folder"],"metadata":{"id":"zWeo8x_quMrw"}}]}